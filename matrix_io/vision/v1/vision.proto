/*
 * Copyright 2016-2017 Matrix Labs
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, version 3 of the License.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */

syntax = "proto3";

package matrix_io.vision.v1;

// google/protobuf protos are Installed along with protoc, see
// https://github.com/google/protobuf/tree/2761122b810fe8861004ae785cc3ab39f384d342/src/google/protobuf
import "google/protobuf/timestamp.proto";

option go_package = "github.com/matrix-io/matrix-protos-go/matrix_io/vision/v1";
option csharp_namespace = "MatrixIO.Vision.V1";
option java_multiple_files = true;
option java_outer_classname = "VisionProto";
option java_package = "one.matrixio.proto.vision.v1";

// Basic point.
message Point {
  // X coordinate.
  float x = 1;
  // Y coordinate.
  float y = 2;
}

message Size {
  // Width (x).
  int32 width = 1;
  // Height (y).
  int32 height = 2;
}

// Basic rectangle. Coordinates are float so that we can return relative
// coordinates when needed.
message Rectangle {
  // Starting X coordinate.
  float x = 1;
  // Starting Y coordinate.
  float y = 2;
  // Width.
  float width = 3;
  // Height.
  float height = 4;
}

enum EnumFacialRecognitionTag {
  FACIAL_RECOGNITION_NOT_DEFINED = 0;
  // Age classification.
  AGE = 1;
  // Emotion classification.
  EMOTION = 2;
  // Gender classification.
  GENDER = 3;
  // Identification.
  FACE_ID = 4;
  // Pose of the head (pitch, yaw, roll).
  HEAD_POSE = 5;
  // Features such as nose, eyes, mouth.
  FACE_FEATURES = 6;
  // Descriptor vector.
  FACE_DESCRIPTOR = 7;
  // Additional face classification 
  // for detections tagged EnumDetectionTag.FACE 
  // permits to discard false possitives
  IS_FACE = 8;
  // Have glass classification.
  HAVE_GLASS = 9;
}

message FacialRecognition {
  // Tags that can be recognized. If a tag starts with "HAS_", it means that
  // another member of the message must be set to specify a value for this tag.

  // What was recognized in the image?
  EnumFacialRecognitionTag tag = 1;

  // Confidence of the recognition.
  float confidence = 2;

  // The following fields should only be present when the tag HAS_AGE tag is set.

  // TODO(nelson.castillo, vision.team): How are we going to represent age?
  // It should be a range.
  int32 age = 3;
  
  enum AgeClass {
    AGE_CLASS_NOT_DEFINED = 0; 
    CHILDREN = 1;
    YOUNG_ADULT = 2;
    ADULT = 3;
    SENIOR = 4;
  }
  
  AgeClass age_class = 13;
  

  // Genders available for detections.
  // TODO(vision.team): Should this be GenderExpression? If that's the case, at some
  // point we might need to add NEUTRAL to the options.
  enum Gender {
    GENDER_NOT_DEFINED = 0;
    MALE = 1;
    FEMALE = 2;
  }

  // The following fields should only be present when the tag HAS_GENDER is set.

  // Detected gender.
  Gender gender = 4;

  // Emotions available for recognition.
  enum Emotion {
    EMOTION_NOT_DEFINED = 0;
    ANGRY = 1;
    DISGUST = 2;
    CONFUSED = 3;
    HAPPY = 4;
    SAD = 5;
    SURPRISED = 6;
    CALM = 7;

    // Feb 9 2018
    FEAR = 8;
    NEUTRAL = 9;
  }

  // The following fields should only be present when the tag HAS_EMOTION is set.

  // Detected emotion.
  Emotion emotion = 5;

  // The following fields should only be present when the tag FACE_DESCRIPTOR is set.

  // Face descriptor. Setting packed=true because of a
  // protobufjs 5.1 bug. packed=true is the default for proto3 syntax.
  repeated float face_descriptor = 6 [packed=true];

  // The following fields should only be present when the tag HAS_FACE_ID is set.

  // Face identification. A string, used to identify a face.
  string face_id = 7;

  // The following fields should only be present when the tag HAS_HEAD_POSE is set.

  // Face yaw.
  float pose_yaw = 8;
  // Race roll.
  float pose_roll = 9;
  // Face pitch.
  float pose_pitch = 10;
  // Face looking
  bool is_looking = 14;

  // The following fields should only be present when the tag HAS_FACE_FEATURES is set.
  // TODO(carlos.gonzalez, nelson.castillo): Improve the features.
  //   Leaving "BasicFaceFeature" for now but this needs to be improved.
  message BasicFaceFeature {
    repeated Point mouth = 1;
    repeated Point left_eye = 2;
    repeated Point right_eye = 3;
    repeated Point nose = 4;
  }

  //  Basic features for the face.
  BasicFaceFeature basic_feature = 11;
  
  // Result of is_face classifier
  bool is_face = 12;
  // Result of have_glass classifier
  bool have_glass = 15;
}

enum ZoneDirection {
  // We can get an NOT_DEFINED direction when the object enters and leaves
  // the zone from the same side or in other edge cases. This enumerator
  // is included here because it is still useful to report the session time
  // of the objects that traverse a zone.
  ZONE_DIRECTION_NOT_DEFINED = 0;
  FORWARD = 1;
  BACKWARD = 2;
}

message ZoneConfig {
  // Zone name.
  string name = 1;
  // First point for the zone. The "FORWARD" direction of a zone is
  // at 90Â° to the right of the line formed by B->A.
  Point point_a = 2;
  // Second point of the zone.
  Point point_b = 3;
  // Total height of the zone. In pixels.
  int32 height = 4;
  // Total heigt in meters. It should be in meters.
  float real_height = 5;
}

message AreaConfig {
  // Area name.
  string name = 1;
  // Points of the area. At least 3 different points.
  repeated Point point = 2;
  // Count direction. The (x,y) direction vector that will be used to 
  // determine whether the object should be counted when leaving the area.
  Point count_direction = 3;
}

message AreaAndZoneConfig {
  // Areas for counting.
  repeated AreaConfig area = 1;
  // Zones for counting.
  repeated ZoneConfig zone = 2;
  // Region of interest, where to detect.
  repeated Rectangle region_of_interest = 3;
}

message VehicleConfig {
  // Camera URL.
  string camera_url = 1;
  // Areas and zones.
  AreaAndZoneConfig area_and_zone = 2;
}

enum EventTag {
  EVENT_NOT_DEFINED = 0;
  TRACKING_START = 1;
  TRACKING_END = 2;
  AREA_ENTER = 3;
  AREA_EXIT = 4;
  ZONE_ENTER = 5;
  ZONE_EXIT = 6;
}

message VisionEvent {
  // Tag for events. The fields below will make sense for a specific tag.
  EventTag tag = 1;

  // Object identifier.
  uint64 tracking_id = 2;

  // Session time: Amount of seconds in tracker
  // Used by TRACKING_END, AREA_EXIT and ZONE_EXIT.
  float session_time = 3;

  // Dwell time: Amount of seconds facing the camera. Used for TRACKING_END event.
  // In seconds.
  float dwell_time = 4;

  // Area identifier. Used when AREA_ENTER or AREA_EXIT is set.
  uint64 area_id = 5;

  // Zone identifier. Used when ZONE_ENTER or ZONE_EXIT is set.
  uint64 zone_id = 6;

  // Direction of the object that leaves the zone.
  // Used when ZONE_ENTER or ZONE_EXIT is set.
  ZoneDirection zone_direction = 7;

  // Speed of the object. Usually relative to the zone.
  float speed = 8;

  // Vision event UTC timestamp (seconds and nanos) 
  // see google/protobuf/timestamp.proto
  google.protobuf.Timestamp timestamp = 9;

  // Friendly identifier for areas. Used when AREA_ENTER or AREA_EXIT is set
  string area_name = 10;

  // Friendly identifier for zones. Used when ZONE_ENTER or ZONE_EXIT is set
  string zone_name = 11;
}

// When a detection is done, a tag specifies what is being detected.
enum EnumDetectionTag {
  DETECTION_NOT_DEFINED = 0;
  FACE = 1;
  HAND_THUMB = 2;
  HAND_PALM = 3;
  HAND_PINCH = 4;
  HAND_FIST = 5;
  PERSON = 6;
}

enum EnumDetectionAlgorithm {
  DETECTION_ALGORITHM_NOT_DEFINED = 0;
  // Default algorithm.
  DEFAULT = 1;
  // First alternative.
  FIRST_ALTERNATIVE = 2;
}

message VehicleRecognition {
  // Type of vehicle: van, motorcyle, etc.
  string type = 1;
  // Brand of car: Toyota, Tesla, etc.
  string brand = 2;
  // Model of the car: Corolla, S, etc.
  string model = 3;
  // Year of release.
  int32 year = 4;
}

// Result of a rectangle detector.
message RectangularDetection {
  // The algorithm used in this detection.
  EnumDetectionAlgorithm algorithm = 1;
  // Location of the detection. Starting at top-left.
  Rectangle location = 2;
  // What kind of detections the rectangle contains.
  EnumDetectionTag tag = 3;
  // Detection confidence.
  float confidence = 4;

  // Facial recognitions for this detection (age, gender, pose, features, etc).
  repeated FacialRecognition facial_recognition = 5;

  // Vehicle recognition for this detection. In most cases you will only
  // get one recognition but we leave room for more than one.
  // This can happen because some cars can have more than type or because
  // we decide to output more than one classifier output.
  repeated VehicleRecognition vehicle_recognition = 9;

  // Speed of the object being detected / tracked.
  // Please use meters per second as the unit.
  float speed = 10;

  // Image snippet of the rectangle taken from the original image where the
  // detection took place.
  bytes image = 6;
  // Image snippet of the rectangle taken from the original image where the
  // detection took place. This image is expected to be smaller than
  // "image".
  bytes image_small = 7;
  // Type of the detection (person, face, car, bus ...)
  string type = 12;
  // Tracking id for this detection.
  uint64 tracking_id = 8;
  // UUID (v4) identifies this unique rectangular detection
  string uuid = 11;
}

// Collection of images. For some applications the
// order can be important for others doesn't.
message ImageList {
  // Image data
  repeated bytes image_data = 1;

  // If image data above was taken in a sequence, the frame rate
  // at which images were taken
  int32 frames_per_second = 2;
}

// Video codecs available for vision requests
enum EnumVideoCodec {
  VIDEO_CODEC_NOT_DEFINED = 0;
  UNDEFINED_VIDEO_CODEC = 1;
  H264 = 2;
  MP4V = 3;
  RV24 = 4;
  VP8 = 5;
  VP9 = 6;
}

// Source video
message Video {
  // RGB video data
  bytes video_data = 1;

  // Video codec used to encode data
  EnumVideoCodec codec = 2;

  // Arbitrary text to tag video content
  // ie. tagging for training purposes
  repeated string tag = 3;
}

enum ImageFormat {
  IMAGE_FORMAT_NOT_DEFINED = 0;
  UNSPECIFIED = 1;
  // unsigned byte, 3 channels. RGB.
  FORMAT_8URGB = 2;
  // unsigned byte, 3 channels. BGR.
  FORMAT_8UBGR = 3;
  // unsigned byte, 1 channel.
  FORMAT_8U = 4;
}

message Image {
  // Image data.
  bytes image = 1;
  // Image format.
  ImageFormat format = 2;
  // Image size.
  Size size = 3;
}

// This message organizes (at least) four components:
// - Detections (faces, hands, cars).
// - Tracking information.
// - Recognition (gender, age, emotion).
//  -Events (New face in video, face leaves video, gesture starts).
message VisionResult {
  // Results of all the rectangular detectors.
  // Each detecion also stores the recognitions that can be done inside of this
  // rectangle. For instance, for faces the available recognitions are
  // (age, gender, emotion).
  repeated RectangularDetection rect_detection = 1;
  // Vision events. For instance, tracking events (start, end).
  // This message is not inside rect_detection because some events will happen when the
  // detection is no longer available, for instance: TRACKING_END.
  repeated VisionEvent vision_event = 4;
  // Source image for the detection. It is mostly useful when you need to keep
  // the source image that was used to generating the detection when you want to
  // evaluate algorithmic changes (test different versions of the same
  // algorithm) or compare different detection algorithms. It is also useful if you want to
  // augment an image with detection results (but not very efficient for this case).
  Image result_image = 5;
  // Same as above but with a resolution that is expected to have less details,
  // i.e.: 640x480 instead of 3840x2160. It could be useful to give the client an idea
  Image result_image_small = 6;

  // Deprecated by result_image.
  bytes image = 2;
  // Deprecated by result_image_small.
  bytes image_small = 3;
  // UUID (v4) that uniquely identifies each vision result
  string uuid = 7;
}
