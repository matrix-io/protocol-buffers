/*
 * Copyright 2016-2017 Matrix Labs
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, version 3 of the License.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */

syntax = "proto3";

package admobilize_vision;

// Basic point.
message Point {
  // X coordinate.
  float x = 1;
  // Y coordinate.
  float y = 2;
}

message Size {
  // Width (x).
  float width = 1;
  // Height (y).
  float height = 2;
}

// Basic rectangle. Coordinates are float so that we can return relative
// coordinates when needed.
message Rectangle {
  // Starting X coordinate.
  float x = 1;
  // Starting Y coordinate.
  float y = 2;
  // Width.
  float width = 3;
  // Height.
  float height = 4;
}

enum EnumFacialRecognitionTag {
  // Age classification.
  AGE = 0;
  // Emotion classification.
  EMOTION = 1;
  // Gender classification.
  GENDER = 2;
  // Identification.
  FACE_ID = 3;
  // Pose of the head (pitch, yaw, roll).
  HEAD_POSE = 4;
  // Features such as nose, eyes, mouth.
  FACE_FEATURES = 5;
  // Descriptor vector.
  FACE_DESCRIPTOR = 6;
}

message FacialRecognition {
  // Tags that can be recognized. If a tag starts with "HAS_", it means that
  // another member of the message must be set to specify a value for this tag.

  // What was recognized in the image?
  EnumFacialRecognitionTag tag = 1;

  // Confidence of the recognition.
  float confidence = 2;

  // The following fields should only be present when the tag HAS_AGE tag is set.

  // TODO(nelson.castillo, vision.team): How are we going to represent age?
  // It should be a range.
  int32 age = 3;

  // Genders available for detections.
  // TODO(vision.team): Should this be GenderExpression? If that's the case, at some
  // point we might need to add NEUTRAL to the options.
  enum Gender {
    MALE = 0;
    FEMALE = 1;
  }

  // The following fields should only be present when the tag HAS_GENDER is set.

  // Detected gender.
  Gender gender = 4;

  // Emotions available for recognition.
  enum Emotion {
    ANGRY = 0;
    DISGUST = 1;
    CONFUSED = 2;
    HAPPY = 3;
    SAD = 4;
    SURPRISED = 5;
    CALM = 6;
  }

  // The following fields should only be present when the tag HAS_EMOTION is set.

  // Detected emotion.
  Emotion emotion = 5;

  // The following fields should only be present when the tag FACE_DESCRIPTOR is set.

  // Face descriptor. Setting packed=true because of a
  // protobufjs 5.1 bug. packed=true is the default for proto3 syntax.
  repeated float face_descriptor = 6 [packed=true];

  // The following fields should only be present when the tag HAS_FACE_ID is set.

  // Face identification. A string, used to identify a face.
  string face_id = 7;

  // The following fields should only be present when the tag HAS_HEAD_POSE is set.

  // Face yaw.
  float pose_yaw = 8;
  // Race roll.
  float pose_roll = 9;
  // Face pitch.
  float pose_pitch = 10;

  // The following fields should only be present when the tag HAS_FACE_FEATURES is set.
  // TODO(carlos.gonzalez, nelson.castillo): Improve the features.
  //   Leaving "BasicFaceFeature" for now but this needs to be improved.
  message BasicFaceFeature {
    repeated Point mouth = 1;
    repeated Point left_eye = 2;
    repeated Point right_eye = 3;
    repeated Point nose = 4;
  }

  //  Basic features for the face.
  BasicFaceFeature basic_feature = 11;
}

enum EventTag {
  TRACKING_START = 0;
  TRACKING_END = 1;
}

message VisionEvent {
  // Tag for events. The fields below will make sense for a specific tag.
  EventTag tag = 1;

  // Object identifier. Used for tracking events (TRACKING_START and TRACKING_END).
  uint64 tracking_id = 2;

  // Session time: Amount of seconds in tracker. Used for TRACKING_END event.
  // In seconds.
  float session_time = 3;
  // Dwell time: Amount of seconds facing the camera. Used for TRACKING_END event.
  // In seconds.
  float dwell_time = 4;
}

// When a detection is done, a tag specifies what is being detected.
enum EnumDetectionTag {
  FACE = 0;
  HAND_THUMB = 1;
  HAND_PALM = 2;
  HAND_PINCH = 3;
  HAND_FIST = 4;
  PERSON = 5;
}

enum EnumDetectionAlgorithm {
    // Default algorithm.
    DEFAULT = 0;
    // First alternative.
    FIRST_ALTERNATIVE = 1;
}

// Result of a rectangle detector.
message RectangularDetection {
  // The algorithm used in this detection.
  EnumDetectionAlgorithm algorithm = 1;
  // Location of the detection. Starting at top-left.
  Rectangle location = 2;
  // What kind of detections the rectangle contains.
  EnumDetectionTag tag = 3;
  // Detection confidence.
  float confidence = 4;
  // Facial recognitions for this detection (age, gender, pose, features, etc).
  repeated FacialRecognition facial_recognition = 5;
  // Image snippet of the rectangle taken from the original image where the
  // detection took place.
  bytes image = 6;
  // Image snippet of the rectangle taken from the original image where the
  // detection took place. This image is expected to be smaller than
  // "image".
  bytes image_small = 7;
  // Tracking id for this detection.
  uint64 tracking_id = 8;
}

// Collection of images. For some applications the 
// order can be important for others doesn't. 
message ImageList {
  // Image data 
  repeated bytes image_data = 1;

  // If image data above was taken in a sequence, the frame rate
  // at which images were taken
  int32 frames_per_second = 2;
}

// Video codecs available for vision requests
enum EnumVideoCodec {
  UNDEFINED_VIDEO_CODEC = 0;
  H264 = 1;
  MP4V = 2;
  RV24 = 3;
  VP8 = 4;
  VP9 = 5;
}

// Source video
message Video {
  // RGB video data
  bytes video_data = 1;

  // Video codec used to encode data
  EnumVideoCodec codec = 2;

  // Arbitrary text to tag video content
  // ie. tagging for training purposes
  repeated string tag = 3;
}

// This message organizes (at least) four components:
// - Detections (faces, hands, cars).
// - Tracking information.
// - Recognition (gender, age, emotion).
//  -Events (New face in video, face leaves video, gesture starts).
message VisionResult {
  // Results of all the rectangular detectors.
  // Each detecion also stores the recognitions that can be done inside of this
  // rectangle. For instance, for faces the available recognitions are 
  // (age, gender, emotion).
  repeated RectangularDetection rect_detection = 1;
  // Vision events. For instance, tracking events (start, end).
  // This message is not inside rect_detection because some events will happen when the
  // detection is no longer available, for instance: TRACKING_END.
  repeated VisionEvent vision_event = 4;
  // Source image for the detection. It is mostly useful when you need to keep
  // the source image that was used to generating the detection when you want to
  // evaluate algorithmic changes (test different versions of the same
  // algorithm) or compare different detection algorithms. It is also useful if you want to
  // augment an image with detection results (but not very efficient for this case).
  bytes image = 2;
  // Size of the image. Useful when sending a raw buffer.
  Size image_size = 5;
  // Same as above but with a resolution that is expected to have less details,
  // i.e.: 640x480 instead of 3840x2160. It could be useful to give the client an idea
  bytes image_small = 3;
  // Size of the small image. Useful when sending a raw buffer.
  Size image_small_size = 6;
}
