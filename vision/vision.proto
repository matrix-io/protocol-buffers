/*
 * Copyright 2016 <Admobilize>
 * All rights reserved.
 */

syntax = "proto3";

package admobilize_vision;

// Basic point.
message Point {
  // X coordinate.
  float x = 1;
  // Y coordinate.
  float y = 2;
}

// Basic rect. Coordinates are float so that we can return relative
// coordinates when needed.
message Rectangle {
  // Starting X coordinate.
  float x = 1;
  // Starting Y coordinate.
  float y = 2;
  // Width.
  float width = 3;
  // Height.
  float height = 4;
}

enum EnumFacialRecognitionTag {
  // Age classification.
  AGE = 0;
  // Emotion classification.
  EMOTION = 1;
  // Gender classification.
  GENDER = 2;
  // Identification.
  FACE_ID = 3;
  // Pose of the head (pitch, yaw, roll).
  HEAD_POSE = 4;
  // Features such as nose, eyes, mouth.
  FACE_FEATURES = 5;
  // Descriptor vector.
  FACE_DESCRIPTOR = 6;
}

message FacialRecognition {
  // Tags that can be recognized. If a tag starts with "HAS_", it means that
  // another member of the message must be set to specify a value for this tag.

  // What was recognized in the image?
  EnumFacialRecognitionTag tag = 1;

  // Confidence of the recognition.
  float confidence = 2;

  // The following fields should only be present when the tag HAS_AGE tag is set.

  // TODO(nelson.castillo, vision.team): How are we going to represent age?
  // It should be a range.
  int32 age = 3;

  // Genders available for detections.
  // TODO(vision.team): Should this be GenderExpression? If that's the case, at some
  // point we might need to add NEUTRAL to the options.
  enum Gender {
    MALE = 0;
    FEMALE = 1;
  }

  // The following fields should only be present when the tag HAS_GENDER is set.

  // Detected gender.
  Gender gender = 4;

  // Emotions available for recognition.
  enum Emotion {
    ANGRY = 0;
    DISGUST = 1;
    CONFUSED = 2;
    HAPPY = 3;
    SAD = 4;
    SURPRISED = 5;
    CALM = 6;
  }

  // The following fields should only be present when the tag HAS_EMOTION is set.

  // Detected emotion.
  Emotion emotion = 5;

  // The following fields should only be present when the tag FACE_DESCRIPTOR is set.

  // Face descriptor.
  repeated float face_descriptor = 6;

  // The following fields should only be present when the tag HAS_FACE_ID is set.

  // Face identification.
  string face_id = 7;

  // The following fields should only be present when the tag HAS_HEAD_POSE is set.

  // Face yaw.
  float pose_yaw = 8;
  // Race roll.
  float pose_roll = 9;
  // Face pitch.
  float pose_pitch = 10;

  // The following fields should only be present when the tag HAS_FACE_FEATURES is set.
  // TODO(carlos.gonzalez, nelson.castillo): Improve the features.
  //   Leaving "BasicFaceFeature" for now but this needs to be improved.
  message BasicFaceFeature {
    repeated Point mouth = 1;
    repeated Point left_eye = 2;
    repeated Point right_eye = 3;
    repeated Point nose = 4;
  }

  //  Basic features for the face.
  BasicFaceFeature basic_feature = 11;
}

// When a detection is done, a tag specifies what is being detected.
enum EnumDetectionTag {
  FACE = 0;

  HAND_THUMB = 1;
  HAND_PALM = 2;
  HAND_PINCH = 3;
  HAND_FIST = 4;
}

enum EnumDetectionAlgorithm {
    // Default algorithm.
    DEFAULT = 0;
    // First alternative.
    FIRST_ALTERNATIVE = 1;
}

// Result of a rectangle detector.
message RectangularDetection {
  // The algorithm used in this detection.
  EnumDetectionAlgorithm algorithm = 1;
  // Location of the detection. Starting at top-left.
  Rectangle location = 2;
  // What kind of detections the rectangle contains.
  EnumDetectionTag tag = 3;
  // Detection confidence.
  float confidence = 4;
  // Facial recognitions for this detection (age, gender, pose, features, etc).
  repeated FacialRecognition facial_recognition = 5;
  // Image snippet of the rectangle taken from the original image where the
  // detection took place.
  bytes image = 6;
  // Image snippet of the rectangle taken from the original image where the
  // detection took place. This image is expected to be smaller than
  // "image".
  bytes image_small = 7;
}

// Collection of images. For some applications the 
// order can be important for others doesn't. 
message ImageList {
  // Image data 
  repeated bytes data = 1;

  // If image data above was taken in a sequence, the frame rate
  // at which images were taken
  int32 frames_per_second = 2;
}

// Video codecs available for vision requests
enum EnumVideoCodec {
  UNDEFINED_VIDEO_CODEC = 0;
  H264 = 1;
  MP4V = 2;
  RV24 = 3;
  VP8 = 4;
  VP9 = 5;
}

// Source video
message Video {
  // RGB video data
  bytes data = 1;

  // Video codec used to encode data
  EnumVideoCodec codec = 2;

  // Arbitrary text to tag video content
  // ie. tagging for training purposes
  repeated string tag = 3;
}

// This message organizes (at least) four components:
// - Detections (faces, hands, cars).
// - Tracking information.
// - Recognition (gender, age, emotion).
//  -Events (New face in video, face leaves video, gesture starts).
message VisionResult {
  // Results of all the rectangular detectors.
  // Each detecion also stores the recognitions that can be done inside of this
  // rectangle. For instance, for faces the available recognitions are 
  // (age, gender, emotion).
  repeated RectangularDetection rect_detection = 1;
  // Source image for the detection. It is mostly useful when you need to keep
  // the source image that was used to generating the detection when you want to
  // evaluate algorithmic changes (test different versions of the same
  // algorithm) or compare different detection algorithms. It is also useful if you want to
  // augment an image with detection results (but not very efficient for this case).
  bytes image = 2;
  // Same as above but with a resolution that is expected to have less details,
  // i.e.: 640x480 instead of 3840x2160. It could be useful to give the client an idea
  bytes image_small = 3;
}
